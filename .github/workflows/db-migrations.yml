name: DB Migrations

on:
  pull_request:
    paths: [ "db/**", "scripts/db/**" ]
  push:
    branches: [ "main" ]
    paths: [ "db/**", "scripts/db/**" ]
  workflow_dispatch:
    inputs:
      target:
        description: Target environment
        required: true
        type: choice
        options: [stg, prd]
      run_lint:
        description: Also run lint-plan on manual run
        required: false
        type: boolean
        default: false
      run_stg:
        description: Also run apply-stg on manual run
        required: false
        type: boolean
        default: false

permissions:
  contents: read

jobs:
  # ---- 1) Gate CI pada PR: status + plan + lint statis ----
  lint-plan:
    if: github.event_name == 'pull_request' || (github.event_name == 'workflow_dispatch' && github.event.inputs.run_lint == 'true')
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: pnpm/action-setup@v3
        with: { version: 10 }
      - uses: actions/setup-node@v4
        with: { node-version: 20, cache: 'pnpm' }
      - run: pnpm install --frozen-lockfile --ignore-scripts

      - name: Status (optional)
        run: |
          if [ -f db/migrate.mjs ]; then
            node db/migrate.mjs --status
          else
            echo "skip: db/migrate.mjs tidak ada (OK)"
          fi

      - name: Lint plan (optional)
        run: |
          if [ -f db/migrate.mjs ]; then
            node db/migrate.mjs --plan
          else
            echo "skip: db/migrate.mjs tidak ada (OK)"
          fi

      - name: Lint migration files (naming, @down, no DROP)
        run: |
          node -e "const fs=require('fs');const p='db/migrations';
          if(!fs.existsSync(p)){process.exit(0)}
          const bad=[];
          for (const f of fs.readdirSync(p).filter(x=>x.endsWith('.sql'))) {
            if(!/^\d{8,}T?\d{0,6}__.+\.sql$/.test(f)) bad.push('bad name: '+f);
            const t=fs.readFileSync(p+'/'+f,'utf8');
            if(!/^([\s\S]*?)--\s*@down\s*$/m.test(t)) bad.push('missing -- @down: '+f);
            if(/\bDROP\s+(TABLE|COLUMN)\b/i.test(t) && !t.includes('/*allow-drop*/')) bad.push('DROP found (no allow): '+f);
          }
          if(bad.length){console.error(bad.join('\n'));process.exit(1);} "

  compat-matrix:
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix: { pg: ['15','16','17'] }

    services:
      postgres:
        image: postgres:${{ matrix.pg }}
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: crive
        ports: ['5432:5432']
        options: >-
          --health-cmd="pg_isready -U postgres -d crive"
          --health-interval=10s --health-timeout=5s --health-retries=30
    
    env:
      DATABASE_URL: postgresql://postgres:postgres@localhost:5432/crive
      
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup pnpm
        uses: pnpm/action-setup@v3
        with:
          version: 10
          
      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: pnpm

      - name: Install deps
        run: pnpm install --frozen-lockfile --ignore-scripts
        
      - name: Apply baseline + migrations (ephemeral DB)
        shell: bash
        run: |
          set -euo pipefail
          if ls sql/up/*.sql >/dev/null 2>&1; then
            for f in $(ls -1 sql/up/*.sql | sort); do
              echo "[baseline] $f"
              psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -f "$f"
            done
          fi
          node db/migrate.mjs --apply --no-advisory-lock

        - name: Schema smoke check
          run: |
            psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -c "select version();"
            psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -c "\dt+"

  # ---- 3) Apply STG manual + Observability artifact ----
  apply-stg:
    if: github.event_name != 'pull_request' && (github.ref == 'refs/heads/main' || github.event.inputs.target == 'stg' || github.event.inputs.run_stg == 'true')
    runs-on: ubuntu-latest
    environment: stg
    env:
      HOST: ${{ secrets.STG_HOST }}
      SSH_KEY_B64: ${{ secrets.STG_SSH_KEY_B64 }}
      DATABASE_URL: ${{ secrets.STG_DATABASE_URL }}
      DOPPLER_TOKEN: ${{ secrets.DOPPLER_TOKEN_STG }}
      DOPPLER_CONFIG: stg
    steps:
      - uses: actions/checkout@v4

      - name: Install psql
        run: sudo apt-get update -y && sudo apt-get install -y postgresql-client

      - name: Install Doppler CLI
        run: curl -sLf https://cli.doppler.com/install.sh | sudo sh

      - name: Install Doppler CLI action
        uses: dopplerhq/cli-action@v3

      - name: Load env from Doppler -> $GITHUB_ENV
        run: |
          set -euo pipefail
          doppler secrets download --no-file --format env \
            --project crive --config "${DOPPLER_CONFIG}" \
            | sed -E 's/^export[[:space:]]+//; s/^;*(.*)$/\1/' >> "$GITHUB_ENV"

      - name: Prepare SSH
        run: |
          set -euo pipefail
          mkdir -p ~/.ssh
          echo "$SSH_KEY_B64" | base64 -d > ~/.ssh/id_ed25519
          chmod 600 ~/.ssh/id_ed25519
          ssh-keyscan -H "$HOST" >> ~/.ssh/known_hosts

      - name: Package SQL
        run: tar -czf migs.tar.gz sql || true

      - name: Upload package
        run: |
          ssh -o StrictHostKeyChecking=yes deploy@"$HOST" 'mkdir -p /deploy/migs'
          scp -o StrictHostKeyChecking=yes migs.tar.gz deploy@"$HOST":/deploy/migs/migs.tar.gz

      - name: Apply SQL (via docker network) — STG + observability
        env:
          OBS_LOG: "migration-${{ github.run_id }}.jsonl"
        run: |
          set -euo pipefail
          RAW_DB="${DATABASE_URL}"; RAW_DB="${RAW_DB%\"}"; RAW_DB="${RAW_DB#\"}"
          DB_URL="${RAW_DB//sslmode=require/sslmode=disable}"
          DB_URL_ESC=$(printf "%s" "$DB_URL" | sed "s/'/'\"'\"'/g")
          scp -o StrictHostKeyChecking=yes migs.tar.gz deploy@"$HOST":/tmp/migs.tar.gz
          ssh -o StrictHostKeyChecking=yes deploy@"$HOST" "DATABASE_URL='$DB_URL_ESC' OBS_LOG='$OBS_LOG' bash -se" <<'REMOTE'
          set -euo pipefail
          rm -rf /deploy/migs/work || true
          mkdir -p /deploy/migs/work
          tar -xzf /tmp/migs.tar.gz -C /deploy/migs/work
          cd /deploy/migs/work
          LOG="/deploy/migs/$OBS_LOG"; : > "$LOG"

          PG_CONT="$(docker ps --format '{{.Names}}\t{{.Ports}}' | awk '/5432/{print $1; exit}' || true)"
          NET=""
          if [ -n "$PG_CONT" ]; then
            NET="$(docker inspect -f '{{range $k,$v := .NetworkSettings.Networks}}{{printf "%s " $k}}{{end}}' "$PG_CONT" | awk '{print $1}')"
          fi

          run_sql() {
            local f="$1"; local rel="${f#$(pwd)/}" ; local s=success; local t0 t1 d
            t0=$(date +%s%3N)
            if [ -n "$NET" ] && docker network inspect "$NET" >/dev/null 2>&1; then
              docker run --rm --network "$NET" -e DATABASE_URL="$DATABASE_URL" -e FPATH="$rel" \
                -v /deploy/migs/work:/work:ro postgres:16-alpine \
                sh -lc 'psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -f "/work/$FPATH"'
            elif docker network inspect crive-stack_default >/dev/null 2>&1; then
              docker run --rm --network crive-stack_default -e DATABASE_URL="$DATABASE_URL" -e FPATH="$rel" \
                -v /deploy/migs/work:/work:ro postgres:16-alpine \
                sh -lc 'psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -f "/work/$FPATH"'
            else
              URL="$DATABASE_URL"
              if echo "$URL" | grep -E '://[^@]*@db(:|/)' >/dev/null 2>&1; then
                URL="$(echo "$URL" | sed 's/@db/@127.0.0.1/')"
              fi
              docker run --rm --network host -e DATABASE_URL="$URL" -e FPATH="$rel" \
                -v /deploy/migs/work:/work:ro postgres:16-alpine \
                sh -lc 'psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -f "/work/$FPATH"'
            fi || s=failure
            t1=$(date +%s%3N); d=$((t1-t0))
            printf '{"ts":"%s","file":"%s","duration_ms":%s,"status":"%s"}\n' \
              "$(date -u +%FT%TZ)" "$rel" "$d" "$s" >> "$LOG"
            [ "$s" = success ]
          }

          if [ -d sql/up ]; then
            for f in $(ls -1 sql/up/*.sql 2>/dev/null | sort); do echo "[apply] $f"; run_sql "$f"; done
          elif [ -f sql/up.sql ]; then
            echo "[apply] sql/up.sql"; run_sql "sql/up.sql"
          elif ls sql/*.sql >/dev/null 2>&1; then
            for f in $(ls -1 sql/*.sql | sort); do echo "[apply] $f"; run_sql "$f"; done
          else
            printf '{"ts":"%s","info":"no_sql_found","status":"noop"}\n' "$(date -u +%FT%TZ)" >> "$LOG"
          fi
          REMOTE

      - name: Fetch observability log (STG)
        env:
          OBS_LOG: "migration-${{ github.run_id }}.jsonl"
        run: scp -o StrictHostKeyChecking=yes deploy@"$HOST":/deploy/migs/"$OBS_LOG" "$OBS_LOG" || true

      - name: Upload observability artifact (STG)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          # ← unik per job & attempt
          name: migration-observability-stg-${{ github.job }}-${{ github.run_attempt }}
          path: "migration-${{ github.run_id }}.jsonl"
          if-no-files-found: ignore


  # ---- 4) Apply STG/PRD manual (tidak jalan di PR) + Snapshot PRD + Observability ----
  migrate:
    if: github.event_name == 'workflow_dispatch' || (github.event_name == 'push' && github.ref == 'refs/heads/main')
    runs-on: ubuntu-latest
    env:
      TARGET: ${{ github.event.inputs.target }}
      HOST: ${{ github.event.inputs.target == 'stg' && secrets.STG_HOST || secrets.PRD_HOST }}
      SSH_KEY_B64: ${{ github.event.inputs.target == 'stg' && secrets.STG_SSH_KEY_B64 || secrets.PRD_SSH_KEY_B64 }}
      DATABASE_URL: ${{ github.event.inputs.target == 'stg' && secrets.STG_DATABASE_URL || secrets.PRD_DATABASE_URL }}
      DOPPLER_TOKEN: ${{ github.event.inputs.target == 'stg' && secrets.DOPPLER_TOKEN_STG || secrets.DOPPLER_TOKEN_PRD }}
      DOPPLER_CONFIG: ${{ github.event.inputs.target == 'stg' && 'stg' || 'prd' }}
    steps:
      - uses: actions/checkout@v4
      - name: Install psql
        run: sudo apt-get update -y && sudo apt-get install -y postgresql-client
      - name: Install Doppler CLI
        run: curl -sLf https://cli.doppler.com/install.sh | sudo sh
      - name: Install Doppler CLI
        uses: dopplerhq/cli-action@v3
      - name: Load env from Doppler → $GITHUB_ENV
        run: |
          set -euo pipefail
          doppler secrets download --no-file --format env \
            --project crive --config "${DOPPLER_CONFIG}" \
          | sed -E 's/^export[[:space:]]+//; s/="(.*)"$/=\1/' >> "$GITHUB_ENV"
      - name: Prepare SSH
        run: |
          set -euo pipefail
          mkdir -p ~/.ssh
          echo "$SSH_KEY_B64" | base64 -d > ~/.ssh/id_ed25519
          chmod 600 ~/.ssh/id_ed25519
          ssh-keyscan -H "$HOST" >> ~/.ssh/known_hosts
      - name: Package SQL
        run: tar -czf migs.tar.gz sql || true
      - name: Upload package
        run: |
          ssh -o StrictHostKeyChecking=yes deploy@"$HOST" 'mkdir -p /deploy/migs'
          scp -o StrictHostKeyChecking=yes migs.tar.gz deploy@"$HOST":/deploy/migs/migs.tar.gz

      - name: Snapshot before migrate (PRD only)
        if: env.TARGET == 'prd'
        env:
          BACKUP_PASSPHRASE: ${{ secrets.BACKUP_PASSPHRASE }}
        run: |
          set -eo pipefail
          BP="${BACKUP_PASSPHRASE-}"
          ssh -o StrictHostKeyChecking=yes deploy@"$HOST" "env DATABASE_URL='${DATABASE_URL}' BACKUP_PASSPHRASE='$BP' bash -se" <<'REMOTE'
          set -euo pipefail
          : "${DATABASE_URL:?missing}"
          [ -z "${BACKUP_PASSPHRASE:-}" ] && BACKUP_PASSPHRASE="$(openssl rand -hex 16)"
          mkdir -p /deploy/backups
          OUT="/deploy/backups/crive-$(date +%Y%m%d-%H%M).dump"
          pg_dump --no-owner --format=custom "$DATABASE_URL" -f "$OUT"
          gpg --batch --yes --passphrase "$BACKUP_PASSPHRASE" -c "$OUT"
          shasum -a 256 "$OUT.gpg" > "$OUT.gpg.sha256"
          rm -f "$OUT"
          echo "Snapshot OK: $OUT.gpg"
          REMOTE

      - name: Apply SQL (via docker network) + observability
        env:
          OBS_LOG: "migration-${{ github.run_id }}.jsonl"
        run: |
          set -euo pipefail
          RAW_DB="${DATABASE_URL}"; RAW_DB="${RAW_DB%\"}"; RAW_DB="${RAW_DB#\"}"
          DB_URL="${RAW_DB//sslmode=require/sslmode=disable}"
          DB_URL_ESC=$(printf "%s" "$DB_URL" | sed "s/'/'\"'\"'/g")
          scp -o StrictHostKeyChecking=yes migs.tar.gz deploy@"$HOST":/tmp/migs.tar.gz
          ssh -o StrictHostKeyChecking=yes deploy@"$HOST" "DATABASE_URL='$DB_URL_ESC' OBS_LOG='$OBS_LOG' bash -se" <<'REMOTE'
          set -euo pipefail
          mkdir -p /deploy/migs && rm -rf /deploy/migs/work || true
          cd /deploy/migs
          tar -xzf /tmp/migs.tar.gz
          LOG="/deploy/migs/$OBS_LOG"; : > "$LOG"

          run_sql() {
            local f="$1"; local rel="${f#$(pwd)/}" ; local s=success; local t0 t1 d
            t0=$(date +%s%3N)
            if docker network inspect crive-stack_default >/dev/null 2>&1; then
              docker run --rm --network crive-stack_default -e DATABASE_URL="$DATABASE_URL" -e FPATH="$rel" \
                -v "$PWD":/work:ro postgres:16-alpine \
                sh -lc 'psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -f "/work/$FPATH"'
            else
              URL="$DATABASE_URL"
              if echo "$URL" | grep -E '://[^@]*@db(:|/)' >/dev/null 2>&1; then
                URL="$(echo "$URL" | sed 's/@db/@127.0.0.1/')"
              fi
              docker run --rm --network host -e DATABASE_URL="$URL" -e FPATH="$rel" \
                -v "$PWD":/work:ro postgres:16-alpine \
                sh -lc 'psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -f "/work/$FPATH"'
            fi || s=failure
            t1=$(date +%s%3N); d=$((t1-t0))
            printf '{"ts":"%s","file":"%s","duration_ms":%s,"status":"%s"}\n' \
              "$(date -u +%FT%TZ)" "$rel" "$d" "$s" >> "$LOG"
            [ "$s" = success ]
          }

          if ls sql/up/*.sql >/dev/null 2>&1; then
            for f in $(ls -1 sql/up/*.sql | sort); do echo "[apply] $f"; run_sql "$f"; done
          elif [ -f sql/up.sql ]; then
            echo "[apply] sql/up.sql"; run_sql "sql/up.sql"
          elif ls sql/*.sql >/dev/null 2>&1; then
            for f in $(ls -1 sql/*.sql | sort); do echo "[apply] $f"; run_sql "$f"; done
          else
            printf '{"ts":"%s","info":"no_sql_found","status":"noop"}\n' "$(date -u +%FT%TZ)" >> "$LOG"
          fi
          REMOTE

      - name: Fetch observability log (STG/PRD)
        env:
          OBS_LOG: "migration-${{ github.run_id }}.jsonl"
        run: scp -o StrictHostKeyChecking=yes deploy@"$HOST":/deploy/migs/"$OBS_LOG" "$OBS_LOG" || true

      - name: Upload observability artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          # ← unik per target, job, attempt
          name: migration-observability-${{ env.TARGET || 'auto' }}-${{ github.job }}-${{ github.run_attempt }}
          path: "migration-${{ github.run_id }}.jsonl"
          if-no-files-found: ignore
